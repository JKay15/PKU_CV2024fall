{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from torchvision.models import VGG19_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # content on conv4_2\n",
    "        # style on conv1_1, conv2_1, conv3_1, conv4_1, conv5_1\n",
    "        self.select = ['0', '5', '10', '19', '28', '21']\n",
    "        vgg = models.vgg19(weights=VGG19_Weights.DEFAULT).features  # pretrained VGG19 model\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        self.vgg = nn.Sequential(*[self.replace_layers(layer) for layer in vgg[:29]])\n",
    "    \n",
    "    def replace_layers(self, layer):\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            return nn.AvgPool2d(kernel_size=layer.kernel_size, stride=layer.stride, padding=layer.padding)\n",
    "        return layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x)\n",
    "        return features\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "        image_path,\n",
    "        transform=None,\n",
    "        device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    read an image\n",
    "    process it to tensor available for vgg\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "def get_features(image, model):\n",
    "    \"\"\"\n",
    "    get feature vectors\n",
    "    \"\"\"\n",
    "    layers = model(image)\n",
    "    features = []\n",
    "    for layer in layers:\n",
    "        feature = layer.reshape(layer.shape[1], -1)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    c , hw = tensor.size()\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram / hw\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    image_size = 224\n",
    "    img_path = './img/'\n",
    "    save_path = './results/'\n",
    "    content_path = f'{img_path}content1.jpg'\n",
    "    style_path = f'{img_path}style1.jpg'\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])]),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),\n",
    "            (1, 1, 1)\n",
    "        ),\n",
    "        transforms.Lambda(lambda x: x.mul_(255))\n",
    "    ])\n",
    "\n",
    "    postprocess = transforms.Compose([\n",
    "        transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "        transforms.Normalize(\n",
    "            mean=[-0.485, -0.456, -0.406],\n",
    "            std=[1, 1, 1]\n",
    "        ),\n",
    "        transforms.Lambda(lambda x: x[torch.LongTensor([2, 1, 0])]),\n",
    "        transforms.Lambda(lambda x: torch.clamp(x, 0, 1)),\n",
    "        transforms.ToPILImage()\n",
    "    ])\n",
    "\n",
    "    # load content and style image\n",
    "    content = preprocess(\n",
    "        content_path,\n",
    "        transform=transform,\n",
    "        device=device\n",
    "    )\n",
    "    style = preprocess(\n",
    "        style_path,\n",
    "        transform=transform,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    model = VGGNet().to(device).eval()\n",
    "\n",
    "    # get content and style features\n",
    "    content_features = get_features(content, model)[-1]\n",
    "    style_features = get_features(style, model)[:-1]\n",
    "    style_grams = [gram_matrix(feature) for feature in style_features]\n",
    "\n",
    "    # white noise image\n",
    "    target = torch.randn_like(content).requires_grad_(True).to(device)\n",
    "    #n target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    content_weight = 1\n",
    "    style_weights = [1e3 / n ** 2 for n in [64, 128, 256, 512, 512]]\n",
    "    optimizer = optim.LBFGS([target])\n",
    "    steps = 500\n",
    "    show_every = 10\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        target_features = get_features(target, model)\n",
    "        content_loss = F.mse_loss(target_features[-1], content_features) * content_weight\n",
    "        style_loss = 0\n",
    "        for target_style, style_gram, style_weight in zip(\n",
    "                target_features[:-1],\n",
    "                style_grams,\n",
    "                style_weights\n",
    "        ):\n",
    "            target_gram = gram_matrix(target_style)\n",
    "            style_loss += F.mse_loss(target_gram, style_gram) * style_weight\n",
    "        total_loss = content_loss + style_loss\n",
    "        total_loss.backward()\n",
    "        return total_loss\n",
    "\n",
    "    for i in tqdm(range(1, steps + 1)):\n",
    "        optimizer.step(closure)\n",
    "        if i % show_every == 0:\n",
    "            loss = closure()\n",
    "            plt.imshow(postprocess(target[0].cpu().detach()))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Iter {i}, Loss: {loss.item()}\")\n",
    "            plt.show()\n",
    "\n",
    "    # save image\n",
    "    plt.imshow(postprocess(target[0].cpu().detach()))\n",
    "    plt.axis(\"off\")\n",
    "    content_name = content_path.split('/')[-1].split('.')[0]\n",
    "    style_name = style_path.split('/')[-1].split('.')[0]\n",
    "    plt.savefig(f'{save_path}{content_name}_{style_name}.png')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
