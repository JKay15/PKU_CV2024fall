{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-21T09:03:29.694720300Z",
     "start_time": "2024-12-21T09:03:15.526054100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # content on conv4_2\n",
    "        # style on conv1_1, conv2_1, conv3_1, conv4_1, conv5_1\n",
    "        self.select = ['0', '5', '10', '19', '28', '20']\n",
    "        vgg = models.vgg19(weights=True).features  # pretrained VGG19 model\n",
    "        for param in vgg.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        net = []\n",
    "        for layer in vgg:\n",
    "            net += self.replace_layers(layer)\n",
    "        self.vgg = nn.Sequential(*net)\n",
    "    \n",
    "    def replace_layers(self, layer):\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            return [nn.AvgPool2d(kernel_size=layer.kernel_size, stride=layer.stride, padding=layer.padding)]\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            return []\n",
    "        return [layer]\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        for name, layer in self.vgg._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in self.select:\n",
    "                features.append(x / x.mean())\n",
    "        return features\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "        image_path,\n",
    "        max_size=None,\n",
    "        shape=None,\n",
    "        transform=None,\n",
    "        device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    read an image\n",
    "    process it to tensor available for vgg\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    if max_size:\n",
    "        scale = max_size / max(image.size)\n",
    "        size = np.array(image.size) * scale\n",
    "        image = image.resize(size.astype(int), Image.ANTIALIAS)\n",
    "    if shape:\n",
    "        image = image.resize(shape, Image.LANCZOS)\n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    return image.to(device)\n",
    "\n",
    "\n",
    "def postprocess(tensor):\n",
    "    \"\"\"\n",
    "    convert a tensor to an image\n",
    "    \"\"\"\n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def get_features(image, model):\n",
    "    \"\"\"\n",
    "    get feature vectors\n",
    "    \"\"\"\n",
    "    layers = model(image)\n",
    "    features = []\n",
    "    for layer in layers:\n",
    "        feature = layer.reshape(layer.shape[1], -1)\n",
    "        features.append(feature)\n",
    "    return features\n",
    "\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    image_size = 356\n",
    "    img_path = './img/'\n",
    "    save_path = './results/'\n",
    "    content_path = f'{img_path}content1.jpg'\n",
    "    style_path = f'{img_path}style.jpg'\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),\n",
    "            (0.229, 0.224, 0.225)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    # load content and style image\n",
    "    content = preprocess(\n",
    "        content_path,\n",
    "        shape=[image_size, image_size],\n",
    "        transform=transform,\n",
    "        device=device\n",
    "    )\n",
    "    style = preprocess(\n",
    "        style_path,\n",
    "        shape=[image_size, image_size],\n",
    "        transform=transform,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    model = VGGNet().to(device).eval()\n",
    "\n",
    "    # get content and style features\n",
    "    content_features = get_features(content, model)\n",
    "    style_features = get_features(style, model)\n",
    "    style_grams = [gram_matrix(feature[:-1]) for feature in style_features]\n",
    "\n",
    "    # white noise image\n",
    "    target = torch.randn_like(content).requires_grad_(True).to(device)\n",
    "\n",
    "    w = 1/5  # w_l\n",
    "    alpha = 1  # alpha\n",
    "    beta = 1e6  # beta\n",
    "    content_weight = 1 / 2\n",
    "    style_weights = [1 / (f.shape[0]**2 * f.shape[1]**2) / 4 for f in style_features]\n",
    "    optimizer = optim.LBFGS([target], lr=0.5)\n",
    "    steps = 500\n",
    "    show_every = 10\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        target_features = get_features(target, model)\n",
    "        content_loss = F.mse_loss(target_features[-1], content_features[-1], reduction='sum') * content_weight\n",
    "        style_loss = 0\n",
    "        for target_feature, style_gram, weight in zip(\n",
    "                target_features,\n",
    "                style_grams,\n",
    "                style_weights\n",
    "        ):\n",
    "            target_gram = gram_matrix(target_feature[:-1])\n",
    "            style_loss += F.mse_loss(target_gram, style_gram, reduction='sum') * weight * w\n",
    "        total_loss = alpha * content_loss + beta * style_loss\n",
    "        total_loss.backward()\n",
    "        return total_loss\n",
    "\n",
    "    for i in tqdm(range(1, steps + 1)):\n",
    "        optimizer.step(closure)\n",
    "        loss = closure()\n",
    "        if i % show_every == 0:\n",
    "            plt.imshow(postprocess(target))\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Iter {i}, Loss: {loss.item()}\")\n",
    "            plt.show()\n",
    "\n",
    "    # save image\n",
    "    plt.imshow(postprocess(target))\n",
    "    plt.axis(\"off\")\n",
    "    content_name = content_path.split('/')[-1].split('.')[0]\n",
    "    style_name = style_path.split('/')[-1].split('.')[0]\n",
    "    plt.savefig(f'{save_path}{content_name}_{style_name}.png')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
