{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer by Relaxed Optimal Transport and Self-Similarity + Auto mask pyramid\n",
    "\n",
    "import the module we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.2.0+cu118\n",
      "Torchvision Version:  0.17.0+cu118\n",
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from imageio import imread\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import PIL\n",
    "from time import time\n",
    "from Patch_Match import PatchMatch\n",
    "\n",
    "patch_match_feat_layer=29\n",
    "AUTO_MASK_PYRAMID=False\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using the GPU!\")\n",
    "elif torch.backends.mps.is_available():\n",
    "  print(\"Using the MPS!\")\n",
    "else:\n",
    "  print(\"WARNING: Could not find GPU! Using CPU only. If you want to enable GPU, please to go Edit > Notebook Settings > Hardware Accelerator and select GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor and PIL utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_random=True\n",
    "\n",
    "# load PIL image\n",
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = PIL.Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "# tensor插值\n",
    "def tensor_resample(tensor, dst_size, mode='bilinear'):\n",
    "    return F.interpolate(tensor, dst_size, mode=mode, align_corners=False)\n",
    "# image resize by short edge\n",
    "def pil_resize_short_edge_to(pil, trg_size):\n",
    "    short_w = pil.width < pil.height\n",
    "    ar_resized_short = (trg_size / pil.width) if short_w else (trg_size / pil.height)\n",
    "    resized = pil.resize((int(pil.width * ar_resized_short), int(pil.height * ar_resized_short)), PIL.Image.BICUBIC)\n",
    "    return resized\n",
    "# image resize by long edge\n",
    "def pil_resize_long_edge_to(pil, trg_size):\n",
    "    short_w = pil.width < pil.height\n",
    "    ar_resized_long = (trg_size / pil.height) if short_w else (trg_size / pil.width)\n",
    "    resized = pil.resize((int(pil.width * ar_resized_long), int(pil.height * ar_resized_long)), PIL.Image.BICUBIC)\n",
    "    return resized\n",
    "\n",
    "def np_to_pil(npy):\n",
    "    return PIL.Image.fromarray(npy.astype(np.uint8))\n",
    "\n",
    "def pil_to_np(pil):\n",
    "    return np.array(pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_np(tensor, cut_dim_to_3=True):\n",
    "    if len(tensor.shape) == 4:\n",
    "        if cut_dim_to_3:\n",
    "            tensor = tensor[0]\n",
    "        else:\n",
    "            return tensor.detach().cpu().numpy().transpose((0, 2, 3, 1))\n",
    "    return tensor.detach().data.cpu().numpy().transpose((1,2,0))\n",
    "\n",
    "def np_to_tensor(npy, space):\n",
    "    if space == 'vgg':\n",
    "        return np_to_tensor_correct(npy)\n",
    "    return (torch.Tensor(npy.astype(np.float64) / 127.5) - 1.0).permute((2,0,1)).unsqueeze(0)\n",
    "\n",
    "def np_to_tensor_correct(npy):\n",
    "    pil = np_to_pil(npy)\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(pil).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplacian Pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplacian(x):\n",
    "    # x - upsample(downsample(x))\n",
    "    return x - tensor_resample(tensor_resample(x, [x.shape[2] // 2, x.shape[3] // 2]), [x.shape[2], x.shape[3]])\n",
    "\n",
    "def make_laplace_pyramid(x, levels):\n",
    "    pyramid = []\n",
    "    current = x\n",
    "    for i in range(levels):\n",
    "        pyramid.append(laplacian(current))\n",
    "        current = tensor_resample(current, (max(current.shape[2] // 2,1), max(current.shape[3] // 2,1)))\n",
    "    pyramid.append(current)\n",
    "    return pyramid\n",
    "\n",
    "def fold_laplace_pyramid(pyramid):\n",
    "    current = pyramid[-1]\n",
    "    for i in range(len(pyramid)-2, -1, -1): # iterate from len-2 to 0\n",
    "        up_h, up_w = pyramid[i].shape[2], pyramid[i].shape[3]\n",
    "        current = pyramid[i] + tensor_resample(current, (up_h,up_w))\n",
    "    return current"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_indices(feat_content, feat_style_all, r, ri, xx, xy, yx):\n",
    "\n",
    "    indices = None\n",
    "    const = 128**2 # 32k or so\n",
    "\n",
    "    feat_style =  feat_style_all[ri]\n",
    "\n",
    "    for i in range(len(feat_style)):\n",
    "        \n",
    "        feat_cont = feat_content[i]\n",
    "        d = feat_style[i].size(1)\n",
    "        feat_style_st = feat_style[i].view(1,d,-1,1)\n",
    "        big_size = feat_cont.shape[2] * feat_cont.shape[3] # num feaxels\n",
    "\n",
    "        stride_x = int(max(math.floor(math.sqrt(big_size//const)),1))\n",
    "        offset_x = np.random.randint(stride_x)\n",
    "        stride_y = int(max(math.ceil(math.sqrt(big_size//const)),1))\n",
    "        offset_y = np.random.randint(stride_y)\n",
    "        xx_arr, xy_arr = np.meshgrid(np.arange(feat_cont.shape[2])[offset_x::stride_x], np.arange(feat_cont.shape[3])[offset_y::stride_y])\n",
    "\n",
    "        xx_arr = np.expand_dims(xx_arr.flatten(),1)\n",
    "        xy_arr = np.expand_dims(xy_arr.flatten(),1)\n",
    "        xc = np.concatenate([xx_arr,xy_arr], 1)\n",
    "\n",
    "        region_mask = r\n",
    "\n",
    "        try:\n",
    "            xc = xc[region_mask[xy_arr[:,0],xx_arr[:,0]], :]\n",
    "        except:\n",
    "            region_mask = region_mask[:,:]\n",
    "            xc = xc[region_mask[xy_arr[:,0],xx_arr[:,0]], :]\n",
    "        \n",
    "        xx[ri].append(xc[:,0])\n",
    "        xy[ri].append(xc[:,1])\n",
    "\n",
    "        feat_result = np.arange(feat_style_st.size(2)).astype(np.int32)\n",
    "        yx[ri].append(feat_result)\n",
    "\n",
    "def get_feature_indices(xx_dict, xy_dict, yx_dict, ri=0, i=0, cnt=32**2):\n",
    "\n",
    "    xx = xx_dict[ri][i][:cnt]\n",
    "    xy = xy_dict[ri][i][:cnt]\n",
    "    yx = yx_dict[ri][i][:cnt]\n",
    "\n",
    "    return xx, xy, yx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract spatial features from VGG layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_feature_extract(feat_result, feat_content, xx, xy):\n",
    "\n",
    "    l2, l3 = [], []\n",
    "    device = feat_result[0].device\n",
    "\n",
    "    # for each extracted layer\n",
    "    for i in range(len(feat_result)):\n",
    "        fr = feat_result[i]\n",
    "        fc = feat_content[i]\n",
    "\n",
    "        # hack to detect reduced scale\n",
    "        if i>0 and feat_result[i-1].size(2) > feat_result[i].size(2):\n",
    "            xx = xx/2.0\n",
    "            xy = xy/2.0\n",
    "\n",
    "\n",
    "        # go back to ints and get residual\n",
    "        xxm = np.floor(xx).astype(np.float32)\n",
    "        xxr = xx - xxm\n",
    "\n",
    "        xym = np.floor(xy).astype(np.float32)\n",
    "        xyr = xy - xym\n",
    "\n",
    "        # do bilinear resample\n",
    "        w00 = torch.from_numpy((1.-xxr)*(1.-xyr)).float().view(1, 1, -1, 1).to(device)\n",
    "        w01 = torch.from_numpy((1.-xxr)*xyr).float().view(1, 1, -1, 1).to(device)\n",
    "        w10 = torch.from_numpy(xxr*(1.-xyr)).float().view(1, 1, -1, 1).to(device)\n",
    "        w11 = torch.from_numpy(xxr*xyr).float().view(1, 1, -1, 1).to(device)\n",
    "\n",
    "        xxm = np.clip(xxm.astype(np.int32),0,fr.size(2)-1)\n",
    "        xym = np.clip(xym.astype(np.int32),0,fr.size(3)-1)\n",
    "\n",
    "        s00 = xxm*fr.size(3)+xym\n",
    "        s01 = xxm*fr.size(3)+np.clip(xym+1,0,fr.size(3)-1)\n",
    "        s10 = np.clip(xxm+1,0,fr.size(2)-1)*fr.size(3)+(xym)\n",
    "        s11 = np.clip(xxm+1,0,fr.size(2)-1)*fr.size(3)+np.clip(xym+1,0,fr.size(3)-1)\n",
    "\n",
    "        fr = fr.view(1,fr.size(1),fr.size(2)*fr.size(3),1)\n",
    "        fr = fr[:,:,s00,:].mul_(w00).add_(fr[:,:,s01,:].mul_(w01)).add_(fr[:,:,s10,:].mul_(w10)).add_(fr[:,:,s11,:].mul_(w11))\n",
    "\n",
    "        fc = fc.view(1,fc.size(1),fc.size(2)*fc.size(3),1)\n",
    "        fc = fc[:,:,s00,:].mul_(w00).add_(fc[:,:,s01,:].mul_(w01)).add_(fc[:,:,s10,:].mul_(w10)).add_(fc[:,:,s11,:].mul_(w11))\n",
    "\n",
    "        l2.append(fr)\n",
    "        l3.append(fc)\n",
    "\n",
    "    x_st = torch.cat([li.contiguous() for li in l2],1)\n",
    "    c_st = torch.cat([li.contiguous() for li in l3],1)\n",
    "\n",
    "    xx = torch.from_numpy(xx).view(1,1,x_st.size(2),1).float().to(device)\n",
    "    yy = torch.from_numpy(xy).view(1,1,x_st.size(2),1).float().to(device)\n",
    "    \n",
    "    x_st = torch.cat([x_st,xx,yy],1)\n",
    "    c_st = torch.cat([c_st,xx,yy],1)\n",
    "    return x_st, c_st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color and distance helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_yuv(rgb):\n",
    "    C = torch.Tensor([[0.577350,0.577350,0.577350],[-0.577350,0.788675,-0.211325],[-0.577350,-0.211325,0.788675]]).to(rgb.device)\n",
    "    yuv = torch.mm(C,rgb)\n",
    "    return yuv\n",
    "\n",
    "def pairwise_distances_cos(x, y):\n",
    "    x_norm = torch.sqrt((x**2).sum(1).view(-1, 1))\n",
    "    y_t = torch.transpose(y, 0, 1)\n",
    "    y_norm = torch.sqrt((y**2).sum(1).view(1, -1))\n",
    "    dist = 1.-torch.mm(x, y_t)/x_norm/y_norm\n",
    "    return dist\n",
    "\n",
    "def pairwise_distances_sq_l2(x, y):\n",
    "    x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    y_t = torch.transpose(y, 0, 1)\n",
    "    y_norm = (y**2).sum(1).view(1, -1)\n",
    "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "    return torch.clamp(dist, 1e-5, 1e5)/x.size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create mask by ignoring some color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_image(image, ignore_color=[0, 0, 0]):\n",
    "    \"\"\"\n",
    "    Create a mask from an image, where pixels matching the ignore_color are set to 0, and others to 1.\n",
    "\n",
    "    :param image: input image.\n",
    "    :param ignore_color: Color to be ignored, default is black ([0, 0, 0]).\n",
    "    :return: Mask tensor of shape (1, H, W), where 1 indicates important areas and 0 indicates areas to ignore.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the image is grayscale or RGB\n",
    "    if len(image.shape) == 2:  # Grayscale image\n",
    "        mask = image != ignore_color[0]\n",
    "    else:  # RGB image\n",
    "        mask = np.all(image != ignore_color, axis=-1)\n",
    "\n",
    "    mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regions building functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_color_regions(content_path, style_path):\n",
    "    s_regions = imread(style_path).transpose(1,0,2)\n",
    "    c_regions = imread(content_path).transpose(1,0,2)\n",
    "\n",
    "    color_codes,c1 = np.unique(s_regions.reshape(-1, s_regions.shape[2]), axis=0,return_counts=True)\n",
    "    \n",
    "    color_codes = color_codes[c1>10000]\n",
    "\n",
    "    c_out = []\n",
    "    s_out = []\n",
    "\n",
    "    for c in color_codes:\n",
    "        c_expand =  np.expand_dims(np.expand_dims(c,0),0)\n",
    "        \n",
    "        s_mask = np.equal(np.sum(s_regions - c_expand,axis=2),0).astype(np.float32)\n",
    "        c_mask = np.equal(np.sum(c_regions - c_expand,axis=2),0).astype(np.float32)\n",
    "\n",
    "        s_out.append(s_mask)\n",
    "        c_out.append(c_mask)\n",
    "    return [c_out,s_out]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_from_matches(matchs,num_of_maxpool,img1,img2):\n",
    "    # 首先我们要找到feat11、feat22中每一个坐标分别对应了其原图中的感受野是那一片\n",
    "    # 然后regions是我们要返回的区域对应关系\n",
    "    # regions是若干对mask的list，其中每一对mask标出了feat11一个元素对应的原图中感受野，另一个mask标出了feat22一个元素对应的原图中感受野\n",
    "    # 首先要找出对应了相同的x,y的点，然后根据这些点找出对应的感受野\n",
    "    dict={}\n",
    "    for i in range(matchs.shape[0]):\n",
    "        for j in range(matchs.shape[1]):\n",
    "            x1,y1=i,j\n",
    "            x2,y2=matchs[i,j]\n",
    "            if (x2,y2) not in dict:\n",
    "                dict[(x2,y2)]=[]\n",
    "            dict[(x2,y2)].append((x1,y1))\n",
    "    c_out,s_out=[],[]\n",
    "    l=2**num_of_maxpool+2 # 18\n",
    "    for key,v in dict.items():\n",
    "        x2,y2=key\n",
    "        mask1=np.zeros(img1.size)\n",
    "        mask2=np.zeros(img2.size)\n",
    "        mask2[x2*l:min((x2+1)*l,img2.size[0]),y2*l:min((y2+1)*l,img2.size[1])]=1\n",
    "        for x1,y1 in v:\n",
    "            mask1[x1*l:min((x1+1)*l,img1.size[0]),y1*l:min((y1+1)*l,img1.size[1])]=1\n",
    "        c_out.append(mask1)\n",
    "        s_out.append(mask2)\n",
    "    return [c_out,s_out] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute distance of two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distmat(x, y, cos_d=True):\n",
    "    if cos_d:\n",
    "        M = pairwise_distances_cos(x, y)\n",
    "    else:\n",
    "        M = torch.sqrt(pairwise_distances_sq_l2(x, y))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content loss function\n",
    "\n",
    "Formal Definition: Let  $D^{X}$  be the pairwise cosine distance matrix of all feature vectors extracted from  $X^{(t)}$ , and let  $D^{I_{C}}$  represent the corresponding matrix for the content image.\n",
    "\n",
    "The content loss is defined as:\n",
    "$\\mathcal{L}_{\\text{content}}(X, C) = \\frac{1}{n^2} \\sum{i,j} \\left| \\frac{D_{ij}^{X}}{\\sum_i D_{ij}^{X}} - \\frac{D_{ij}^{I_{C}}}{\\sum_i D_{ij}^{I_{C}}} \\right|$\n",
    "\n",
    "Interpretation of this loss: For the normalized cosine distances between feature vectors (row-wise normalization), the values should remain consistent between the content image and the output image.\n",
    "\n",
    "Effect of this loss: This loss preserves the structure of the output image without enforcing direct pixel-wise similarity to the content image. As a result, the semantics and local layout of the content are maintained, while allowing the pixel values in  $X^{(t)}$  to differ significantly from those in the content image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(feat_result, feat_content):\n",
    "    d = feat_result.size(1)\n",
    "\n",
    "    X = feat_result.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "    Y = feat_content.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "\n",
    "    Y = Y[:,:-2]\n",
    "    X = X[:,:-2]\n",
    "    # X = X.t()\n",
    "    # Y = Y.t()\n",
    "\n",
    "    Mx = distmat(X, X)\n",
    "    Mx = Mx#/Mx.sum(0, keepdim=True)\n",
    "\n",
    "    My = distmat(Y, Y)\n",
    "    My = My#/My.sum(0, keepdim=True)\n",
    "\n",
    "    d = torch.abs(Mx-My).mean()# * X.shape[0]\n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Style loss functions\n",
    "\n",
    "Let  $A = \\{ A_1, \\dots, A_n \\}$  represent a set of  n  feature vectors extracted from  $X^{(t)}$ , and  $B = \\{ B_1, \\dots, B_m \\}$  represent a set of  m  feature vectors extracted from the style image  $I_S$ . The style loss is derived from the Earth Mover’s Distance (EMD):\n",
    "\n",
    "\n",
    "$$EMD(A, B) = \\min_{T \\geq 0} \\sum_{i,j} T_{ij} C_{ij}$$\n",
    "\n",
    "- Constraints on  T :$$\\sum_j T_{ij} = \\frac{1}{m}, \\quad \\sum_i T_{ij} = \\frac{1}{n}$$ \n",
    "  - T  is the transport matrix, where  $T_{ij}$  represents the weight of the distance between the  i -th element of  A  and the  j -th element of  B  in the EMD computation.\n",
    "  - $T_{ij}$  is non-negative and is optimized during the computation.\n",
    "\n",
    "\n",
    "- Cost Matrix  C :\n",
    "  - $C_{ij}$  represents the distance between the  i -th element of  A  and the  j -th element of  B .\n",
    "\n",
    "The EMD measures the distance between two sets. However, computing the optimal  T  has a time complexity of  $O(\\max(n, m)^3)$ , which is computationally impractical for gradient-based style transfer.\n",
    "\n",
    "Relaxed EMD:\n",
    "\n",
    "To address this, a relaxed version of the EMD is used. Two auxiliary distances are defined by considering only one of the original constraints at a time:\n",
    "\n",
    "1.\tRelaxed EMD for  A :\n",
    "\n",
    "$$R_A(A, B) = \\min_{T \\geq 0} \\sum_{i,j} T_{ij} C_{ij}, \\quad \\text{s.t.} \\quad \\sum_j T_{ij} = \\frac{1}{m}$$\n",
    "\n",
    "2.\tRelaxed EMD for  B :\n",
    "\n",
    "$$R_B(A, B) = \\min_{T \\geq 0} \\sum_{i,j} T_{ij} C_{ij}, \\quad \\text{s.t.} \\quad \\sum_i T_{ij} = \\frac{1}{n}$$\n",
    "\n",
    "\n",
    "The relaxed EMD is then defined as:\n",
    "\n",
    "$$l_r = REMD(A, B) = \\max(R_A(A, B), R_B(A, B))$$\n",
    "\n",
    "\n",
    "This can be equivalently expressed as:\n",
    "\n",
    "$$l_r = \\max\\left( \\frac{1}{n} \\sum_j \\min_i C_{ij}, \\frac{1}{m} \\sum_i \\min_j C_{ij} \\right)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_loss(X, Y, cos_d=True):\n",
    "    d = X.shape[1]\n",
    "\n",
    "    if d == 3:\n",
    "        X = rgb_to_yuv(X.transpose(0,1).contiguous().view(d,-1)).transpose(0,1)\n",
    "        Y = rgb_to_yuv(Y.transpose(0,1).contiguous().view(d,-1)).transpose(0,1)\n",
    "    else:\n",
    "        X = X.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "        Y = Y.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "\n",
    "    # Relaxed EMD\n",
    "    CX_M = distmat(X, Y, cos_d=True)\n",
    "\n",
    "    if d==3: CX_M = CX_M + distmat(X, Y, cos_d=False)\n",
    "\n",
    "    m1, m1_inds = CX_M.min(1)\n",
    "    m2, m2_inds = CX_M.min(0)\n",
    "\n",
    "    remd = torch.max(m1.mean(), m2.mean())\n",
    "\n",
    "    return remd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moment loss function\n",
    "\n",
    "The relaxed EMD ( $l_r$ ) effectively transfers the structural style of the source image to the target image. However, cosine distance ( $D_{\\cos}$ ) overlooks the magnitudes of feature vectors, leading to visual artifacts in the output, such as oversaturation or undersaturation of colors.\n",
    "\n",
    "To address this issue, a Moment Matching Loss is introduced. This loss improves the visual quality of the output, especially for color saturation and detail retention:\n",
    "\n",
    "\n",
    "$$l_m = \\frac{1}{d} \\| \\mu_A - \\mu_B \\|_1 + \\frac{1}{d^2} \\| \\Sigma_A - \\Sigma_B \\|_1$$\n",
    "\n",
    "- Components:\n",
    "  - $\\mu_A$ : The mean of feature vectors in  A .\n",
    "  - $\\Sigma_A$ : The covariance of feature vectors in  A .\n",
    "  - Similarly,  $\\mu_B$  and  $\\Sigma_B$  are the mean and covariance of feature vectors in  B .\n",
    "  - d : The dimensionality of the feature vectors.\n",
    "- Interpretation:\n",
    "  - The first term ( $\\frac{1}{d} \\| \\mu_A - \\mu_B \\|_1$ ) ensures that the overall distributions of feature vector means for  A  and  B  are aligned.\n",
    "  - The second term ( $\\frac{1}{d^2} \\| \\Sigma_A - \\Sigma_B \\|_1$ ) aligns the variances and correlations within the feature sets of  A  and  B ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moment_loss(X, Y, moments=[1,2]):\n",
    "    loss = 0.\n",
    "    d = X.size(1)\n",
    "    # X = X.squeeze().t()\n",
    "    # Y = Y.squeeze().t()\n",
    "\n",
    "    Xo = X.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "    Yo = Y.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "\n",
    "    splits = [Xo.size(1)]\n",
    "\n",
    "    cb = 0\n",
    "    ce = 0\n",
    "    for i in range(len(splits)):\n",
    "        ce = cb + splits[i]\n",
    "        X = Xo[:,cb:ce]\n",
    "        Y = Yo[:,cb:ce]\n",
    "        cb = ce\n",
    "\n",
    "        mu_x = torch.mean(X,0,keepdim=True)\n",
    "        mu_y = torch.mean(Y,0,keepdim=True)\n",
    "        mu_d = torch.abs(mu_x-mu_y).mean()\n",
    "\n",
    "        if 1 in moments:\n",
    "            # print(mu_x.shape)\n",
    "            loss = loss + mu_d\n",
    "\n",
    "        if 2 in moments:\n",
    "\n",
    "            sig_x = torch.mm((X-mu_x).transpose(0,1), (X-mu_x))/X.size(0)\n",
    "            sig_y = torch.mm((Y-mu_y).transpose(0,1), (Y-mu_y))/Y.size(0)\n",
    "\n",
    "            sig_d = torch.abs(sig_x-sig_y).mean()\n",
    "\n",
    "            # print(X_cov.shape)\n",
    "            # exit(1)\n",
    "            loss = loss + sig_d\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the loss\n",
    "\n",
    "This approach uses gradient descent with RMSprop to minimize the objective function based on the output image  X , given a style image ( $I_S$ ) and a content image ( $I_C$ ):\n",
    "\n",
    "\n",
    "$$L(X, I_C, I_S) = \\frac{\\alpha l_C + l_M + l_r + \\frac{1}{\\alpha} l_p}{2 + \\alpha + \\frac{1}{\\alpha}}$$\n",
    "\n",
    "Components of the Objective Function:\n",
    "1.\tContent Term ( $\\alpha l_C$ ):\n",
    "    - This term ensures that the output image  X  retains the structure and semantic layout of the content image  $I_C$ .\n",
    "    - Weighted by the hyperparameter  $\\alpha$ , which controls the emphasis on preserving content.\n",
    "2.\tStyle Terms ( $l_M + l_r + \\frac{1}{\\alpha} l_p$ ):\n",
    "    - Moment Matching Loss ( $l_M$ ): Helps align the color and detail distribution between  X  and the style image  $I_S$ , addressing issues like oversaturation.\n",
    "    - Relaxed EMD ( $l_r$ ): Ensures structural and style similarity by aligning the feature sets of  X  and  $I_S$ .\n",
    "    - Perceptual Loss ( $l_p$ ): A fine-grained loss term (scaled by  $\\frac{1}{\\alpha}$ ) to further refine the visual fidelity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(feat_result, feat_content, feat_style, xx_dict, xy_dict, yx_dict, content_weight, regions, moment_weight=1.0):\n",
    "    # spatial feature extract\n",
    "    num_locations = 1024\n",
    "    loss_total = 0.\n",
    "\n",
    "    for ri in range(len(regions[0])):\n",
    "        xx, xy, yx = get_feature_indices(xx_dict, xy_dict, yx_dict, ri=ri, cnt=num_locations)\n",
    "        spatial_result, spatial_content = spatial_feature_extract(feat_result, feat_content, xx, xy)\n",
    "\n",
    "        loss_content = content_loss(spatial_result, spatial_content)\n",
    "\n",
    "        d = feat_style[ri][0].shape[0]\n",
    "        spatial_style = feat_style[ri][0].view(1, d, -1, 1)\n",
    "\n",
    "        feat_max = 3+2*64+128*2+256*3+512*2 # (sum of all extracted channels)\n",
    "\n",
    "        loss_remd = style_loss(spatial_result[:, :feat_max, :, :], spatial_style[:, :feat_max, :, :])\n",
    "\n",
    "        loss_moment = moment_loss(spatial_result[:,:-2,:,:], spatial_style, moments=[1,2]) # -2 is so that it can fit?\n",
    "        # palette matching\n",
    "        content_weight_frac = 1./max(content_weight,1.)\n",
    "        loss_moment += content_weight_frac * style_loss(spatial_result[:,:3,:,:], spatial_style[:,:3,:,:])\n",
    "        \n",
    "        loss_style = loss_remd + moment_weight * loss_moment\n",
    "        print(f'Style: {loss_style.item():.3f}, Content: {loss_content.item():.3f}')\n",
    "\n",
    "        style_weight = 1.0 + moment_weight\n",
    "        loss_total += (content_weight * loss_content + loss_style) / (content_weight + style_weight)\n",
    "\n",
    "    return loss_total/len(xx_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: VGG network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network design\n",
    "\n",
    "#### User Control in Style Transfer\n",
    "1.\tIntegrating User Constraints\n",
    "    - User-defined constraints can restrict the output style by enforcing correspondence between two sets of local regions in  $X^{(t)}$  (output image) and  $I_S$  (style image).\n",
    "    - These regions are designed to have low style loss between the corresponding areas.\n",
    "2.\tPoint-to-Point User Control\n",
    "    - Each correspondence involves a pair of single local points, defined as  $(X_{t_1}, S_{s_1}), \\dots, (X_{t_K}, S_{s_K})$ .\n",
    "\n",
    "#### Redefining REMD with User Constraints\n",
    "\n",
    "The cost matrix  $C_{ij}$  in the Relaxed Earth Mover’s Distance (REMD) is redefined as:\n",
    "\n",
    "\n",
    "$$C_{ij} =\n",
    "\\begin{cases}\n",
    "\\beta \\cdot D_{\\cos}(A_i, B_j), & \\text{if } i \\in X_{t_k}, j \\in S_{s_k} \\\\\n",
    "\\infty, & \\text{if } \\exists k \\text{ such that } i \\in X_{t_k}, j \\notin S_{s_k} \\\\\n",
    "D_{\\cos}(A_i, B_j), & \\text{otherwise}.\n",
    "\\end{cases}$$\n",
    "\n",
    "- Explanation:\n",
    "  - $\\beta$  controls the weight of user-specified constraints relative to unconstrained areas.\n",
    "  - In this study,  $\\beta$  is set to 5 in all experiments.\n",
    "  - $D_{\\cos}(A_i, B_j)$  represents the cosine distance between feature vectors  $A_i$  (output image) and  $B_j$  (style image).\n",
    "\n",
    "#### Enhancing Point-to-Point Control\n",
    "\n",
    "- To strengthen the constraints, for each originally defined point, a 3×3 grid of additional constraint points is created around it.\n",
    "- These new constraints include points within a 20-pixel horizontal and vertical distance of the original point in a 512×512 image.\n",
    "- The spacing can be adjusted according to specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESAMPLE_FREQ = 1\n",
    "\n",
    "class Vgg19_Extractor(nn.Module):\n",
    "    def __init__(self, space):\n",
    "        super().__init__()\n",
    "        self.vgg_layers = models.vgg19(weights=models.VGG19_Weights.DEFAULT).features\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.capture_layers = set([1, 3, 6, 8, 11, 13, 15, 17, 20, 22, 24, 26, 29, 31, 33, 35])\n",
    "        self.patch_match_layer=29\n",
    "        self.num_max_pool = 4\n",
    "        self.space = space\n",
    "        \n",
    "    def patch_match_features(self, x):\n",
    "        feat=None\n",
    "        for i in range(len(self.vgg_layers)):\n",
    "            x = self.vgg_layers[i](x)\n",
    "            if i == self.patch_match_layer:\n",
    "                feat = x\n",
    "                return feat/torch.norm(feat)\n",
    "    \n",
    "    def forward_base(self, x):\n",
    "        feat = [x]\n",
    "        for i in range(len(self.vgg_layers)):\n",
    "            x = self.vgg_layers[i](x)\n",
    "            if i in self.capture_layers: feat.append(x)\n",
    "        return feat\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.space != 'vgg':\n",
    "            x = (x + 1.) / 2.\n",
    "            x = x - (torch.Tensor([0.485, 0.456, 0.406]).to(x.device).view(1, -1, 1, 1))\n",
    "            x = x / (torch.Tensor([0.229, 0.224, 0.225]).to(x.device).view(1, -1, 1, 1))\n",
    "        feat = self.forward_base(x)\n",
    "        return feat\n",
    "    \n",
    "    def forward_samples_hypercolumn(self, X, samps=100):\n",
    "        feat = self.forward(X)\n",
    "\n",
    "        xx,xy = np.meshgrid(np.arange(X.shape[2]), np.arange(X.shape[3]))\n",
    "        xx = np.expand_dims(xx.flatten(),1)\n",
    "        xy = np.expand_dims(xy.flatten(),1)\n",
    "        xc = np.concatenate([xx,xy],1)\n",
    "        \n",
    "        samples = min(samps,xc.shape[0])\n",
    "\n",
    "        np.random.shuffle(xc)\n",
    "        xx = xc[:samples,0]\n",
    "        yy = xc[:samples,1]\n",
    "\n",
    "        feat_samples = []\n",
    "        for i in range(len(feat)):\n",
    "\n",
    "            layer_feat = feat[i]\n",
    "\n",
    "            # hack to detect lower resolution\n",
    "            if i>0 and feat[i].size(2) < feat[i-1].size(2):\n",
    "                xx = xx/2.0\n",
    "                yy = yy/2.0\n",
    "\n",
    "            xx = np.clip(xx, 0, layer_feat.shape[2]-1).astype(np.int32)\n",
    "            yy = np.clip(yy, 0, layer_feat.shape[3]-1).astype(np.int32)\n",
    "\n",
    "            features = layer_feat[:,:, xx[range(samples)], yy[range(samples)]]\n",
    "            feat_samples.append(features.clone().detach())\n",
    "\n",
    "        feat = torch.cat(feat_samples,1)\n",
    "        return feat\n",
    "    \n",
    "    def forward_cat(self, X, r, samps=100):\n",
    "        feat = self.forward(X)\n",
    "\n",
    "        try:\n",
    "            r = r[:,:,0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if r.max()<0.1:\n",
    "            region_mask = np.greater(r.flatten()+1.,0.5)\n",
    "        else:\n",
    "            region_mask = np.greater(r.flatten(),0.5)\n",
    "\n",
    "        xx,xy = np.meshgrid(np.arange(X.shape[2]), np.arange(X.shape[3]))\n",
    "        xx = np.expand_dims(xx.flatten(),1)\n",
    "        xy = np.expand_dims(xy.flatten(),1)\n",
    "        xc = np.concatenate([xx,xy],1)\n",
    "\n",
    "        xc = xc[region_mask,:]\n",
    "\n",
    "        samples = min(samps,xc.shape[0])\n",
    "\n",
    "        np.random.shuffle(xc)\n",
    "        xx = xc[:samples,0]\n",
    "        yy = xc[:samples,1]\n",
    "\n",
    "        feat_samples = []\n",
    "        for i in range(len(feat)):\n",
    "            layer_feat = feat[i]\n",
    "            # hack to detect lower resolution\n",
    "            if i>0 and feat[i].size(2) < feat[i-1].size(2):\n",
    "                xx = xx/2.0\n",
    "                yy = yy/2.0\n",
    "            \n",
    "            xx = np.clip(xx, 0, layer_feat.shape[2]-1).astype(np.int32)\n",
    "            yy = np.clip(yy, 0, layer_feat.shape[3]-1).astype(np.int32)\n",
    "\n",
    "            features = layer_feat[:,:, xx[range(samples)], yy[range(samples)]]\n",
    "            feat_samples.append(features.clone().detach())\n",
    "\n",
    "        feat = torch.cat(feat_samples,1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESAMPLE_FREQ = 1\n",
    "\n",
    "class Vgg16_Extractor(nn.Module):\n",
    "    def __init__(self, space):\n",
    "        super().__init__()\n",
    "        self.vgg_layers = models.vgg16(weights=models.VGG16_Weights.DEFAULT).features\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.capture_layers = set([1,3,6,8,11,13,15,22,29])\n",
    "        self.space = space\n",
    "        \n",
    "    def forward_base(self, x):\n",
    "        feat = [x]\n",
    "        for i in range(len(self.vgg_layers)):\n",
    "            x = self.vgg_layers[i](x)\n",
    "            if i in self.capture_layers: feat.append(x)\n",
    "        return feat\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.space != 'vgg':\n",
    "            x = (x + 1.) / 2.\n",
    "            x = x - (torch.Tensor([0.485, 0.456, 0.406]).to(x.device).view(1, -1, 1, 1))\n",
    "            x = x / (torch.Tensor([0.229, 0.224, 0.225]).to(x.device).view(1, -1, 1, 1))\n",
    "        feat = self.forward_base(x)\n",
    "        return feat\n",
    "    \n",
    "    def forward_samples_hypercolumn(self, X, samps=100):\n",
    "        feat = self.forward(X)\n",
    "\n",
    "        xx,xy = np.meshgrid(np.arange(X.shape[2]), np.arange(X.shape[3]))\n",
    "        xx = np.expand_dims(xx.flatten(),1)\n",
    "        xy = np.expand_dims(xy.flatten(),1)\n",
    "        xc = np.concatenate([xx,xy],1)\n",
    "        \n",
    "        samples = min(samps,xc.shape[0])\n",
    "\n",
    "        np.random.shuffle(xc)\n",
    "        xx = xc[:samples,0]\n",
    "        yy = xc[:samples,1]\n",
    "\n",
    "        feat_samples = []\n",
    "        for i in range(len(feat)):\n",
    "\n",
    "            layer_feat = feat[i]\n",
    "\n",
    "            # hack to detect lower resolution\n",
    "            if i>0 and feat[i].size(2) < feat[i-1].size(2):\n",
    "                xx = xx/2.0\n",
    "                yy = yy/2.0\n",
    "\n",
    "            xx = np.clip(xx, 0, layer_feat.shape[2]-1).astype(np.int32)\n",
    "            yy = np.clip(yy, 0, layer_feat.shape[3]-1).astype(np.int32)\n",
    "\n",
    "            features = layer_feat[:,:, xx[range(samples)], yy[range(samples)]]\n",
    "            feat_samples.append(features.clone().detach())\n",
    "\n",
    "        feat = torch.cat(feat_samples,1)\n",
    "        return feat\n",
    "    \n",
    "    def forward_cat(self, X, r, samps=100):\n",
    "        feat = self.forward(X)\n",
    "\n",
    "        try:\n",
    "            r = r[:,:,0]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if r.max()<0.1:\n",
    "            region_mask = np.greater(r.flatten()+1.,0.5)\n",
    "        else:\n",
    "            region_mask = np.greater(r.flatten(),0.5)\n",
    "\n",
    "        xx,xy = np.meshgrid(np.arange(X.shape[2]), np.arange(X.shape[3]))\n",
    "        xx = np.expand_dims(xx.flatten(),1)\n",
    "        xy = np.expand_dims(xy.flatten(),1)\n",
    "        xc = np.concatenate([xx,xy],1)\n",
    "\n",
    "        xc = xc[region_mask,:]\n",
    "\n",
    "        samples = min(samps,xc.shape[0])\n",
    "\n",
    "        np.random.shuffle(xc)\n",
    "        xx = xc[:samples,0]\n",
    "        yy = xc[:samples,1]\n",
    "\n",
    "        feat_samples = []\n",
    "        for i in range(len(feat)):\n",
    "            layer_feat = feat[i]\n",
    "            # hack to detect lower resolution\n",
    "            if i>0 and feat[i].size(2) < feat[i-1].size(2):\n",
    "                xx = xx/2.0\n",
    "                yy = yy/2.0\n",
    "            \n",
    "            xx = np.clip(xx, 0, layer_feat.shape[2]-1).astype(np.int32)\n",
    "            yy = np.clip(yy, 0, layer_feat.shape[3]-1).astype(np.int32)\n",
    "\n",
    "            features = layer_feat[:,:, xx[range(samples)], yy[range(samples)]]\n",
    "            feat_samples.append(features.clone().detach())\n",
    "\n",
    "        feat = torch.cat(feat_samples,1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing Computation for Loss Calculation\n",
    "1.\tRandom Sampling Strategy\n",
    "  - To avoid computing distances between every pixel in the style and output images, a random sampling approach is employed:\n",
    "  - Style Image: Randomly sample 1024 points.\n",
    "  - Content Image: Use a uniform grid to sample 1024 points, adding a random  x, y  offset at each sampling step to cover a broader image area.\n",
    "2.\tEfficiency in Loss Calculation\n",
    "  - Only the feature vectors corresponding to the sampled points are used for loss calculation.\n",
    "  - Gradients (used in backpropagation) are also computed only for these sampled points, avoiding unnecessary optimization over all pixels.\n",
    "3.\tDynamic Sampling\n",
    "  - After each optimization step (using the RMSprop algorithm), the sampling positions are refreshed.\n",
    "  - This dynamic sampling strategy helps:\n",
    "  - Avoid overfitting to specific regions of the image.\n",
    "  - Gradually cover the entire image’s feature space across multiple iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(result, content, style, content_path, style_path, content_weight, lr, extractor, regions=0):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    result_pyramid = make_laplace_pyramid(result, 5)\n",
    "    result_pyramid = [l.data.requires_grad_() for l in result_pyramid]\n",
    "\n",
    "    opt_iter = 200\n",
    "\n",
    "    # use rmsprop\n",
    "    optimizer = optim.RMSprop(result_pyramid, lr=lr)\n",
    "\n",
    "    # extract features for content\n",
    "    feat_content = extractor(content) # \n",
    "\n",
    "    stylized = fold_laplace_pyramid(result_pyramid)\n",
    "    \n",
    "    # some inner loop that extracts samples\n",
    "    feat_style_all = []\n",
    "    for ri in range(len(regions[0])):\n",
    "        r_temp = regions[0][ri]\n",
    "        if len(r_temp.shape) > 2:\n",
    "            r_temp = r_temp[:,:,0]\n",
    "        \n",
    "        r_temp = torch.from_numpy(r_temp).unsqueeze(0).unsqueeze(0).contiguous()\n",
    "        if AUTO_MASK_PYRAMID:\n",
    "            r=r_temp[0,0,:,:].numpy()\n",
    "        else:\n",
    "            r = tensor_resample(r_temp,[style.size(3),style.size(2)])[0,0,:,:].numpy()\n",
    "        feat_style = None\n",
    "        for j in range(5):\n",
    "            with torch.no_grad():\n",
    "                feat_e = extractor.forward_cat(style, r, samps=1000)  \n",
    "                feat_style = feat_e if feat_style is None else torch.cat((feat_style, feat_e), dim=2)\n",
    "        feat_style_all.append(feat_style)\n",
    "\n",
    "    xx = {}\n",
    "    xy = {}\n",
    "    yx = {}\n",
    "        \n",
    "    for ri in range(len(regions[0])):\n",
    "\n",
    "        try:\n",
    "            temp = xx[ri]\n",
    "        except:\n",
    "            xx[ri] = []\n",
    "            xy[ri] = []\n",
    "            yx[ri] = []\n",
    "\n",
    "        r_temp = regions[0][ri]\n",
    "        r_temp = torch.from_numpy(r_temp).unsqueeze(0).unsqueeze(0).contiguous()\n",
    "        if AUTO_MASK_PYRAMID:\n",
    "            r=r_temp[0,0,:,:].numpy()\n",
    "        else:\n",
    "            r = tensor_resample(r_temp, ([stylized.size(3), stylized.size(2)]))[0,0,:,:].numpy()     \n",
    "\n",
    "        if r.max()<0.1:\n",
    "            r = np.greater(r+1.,0.5)\n",
    "        else:\n",
    "            r = np.greater(r,0.5)\n",
    "   \n",
    "        sample_indices(feat_content, feat_style_all, r, ri, xx, xy, yx) # 0 to sample over first layer extracted\n",
    "\n",
    "    # init indices to optimize over\n",
    "    for it in range(opt_iter):\n",
    "        optimizer.zero_grad()\n",
    "        stylized = fold_laplace_pyramid(result_pyramid)\n",
    "        # original code has resample here, seems pointless with uniform shuffle\n",
    "        # ...\n",
    "        # also shuffle them every y iter\n",
    "        if it % 1 == 0 and it != 0:\n",
    "            for ri in xx.keys():\n",
    "                np.random.shuffle(xx[ri][0])\n",
    "                np.random.shuffle(xy[ri][0])\n",
    "                np.random.shuffle(yx[ri][0])\n",
    "\n",
    "        feat_result = extractor(stylized)\n",
    "\n",
    "        loss = calculate_loss(feat_result, feat_content, feat_style_all, xx, xy, yx, content_weight, regions)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return stylized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: STROTSS process\n",
    "\n",
    "1.\tIterative Resolution Adjustment\n",
    "    - The method iteratively increases the resolution and halves the value of  \\alpha  in each step.\n",
    "2.\tInitial Resolution\n",
    "    - At the start, the longer side of both the content image and style image is scaled down to 64 pixels.\n",
    "    - The output from each scaled resolution is bilinearly upsampled to twice the resolution and used as the initialization for the next scale’s output.\n",
    "3.\tDefault Settings\n",
    "    - By default, the process iterates four times.\n",
    "    - The initial value of  \\alpha = 16 , ensuring that  \\alpha = 1  at the final resolution.\n",
    "4.\tInitialization at the Lowest Resolution\n",
    "    - At the lowest resolution:\n",
    "    - The output is initialized as the lowest level of the Laplacian pyramid of the content image, combined with the average color of the style image.\n",
    "    - This initialized image is then decomposed into a 5-level Laplacian pyramid.\n",
    "5.\tOptimization Using RMSprop\n",
    "    - RMSprop is used to update the components of the Laplacian pyramid, minimizing the objective function.\n",
    "    - Optimizing the Laplacian pyramid instead of raw pixels significantly accelerates convergence.\n",
    "6.\tOptimization Details\n",
    "    - For each resolution:\n",
    "    - RMSprop is run for 200 updates.\n",
    "    - The learning rate is set to 0.002 for all resolutions except the last one, where it is reduced to 0.001."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strotss(content_pil, style_pil, content_path, style_path, regions, content_weight=1.0*16.0, device='cuda:0', space='uniform', content_mask=None, style_mask=None):\n",
    "    content_np = pil_to_np(content_pil)\n",
    "    style_np = pil_to_np(style_pil)\n",
    "    content_full = np_to_tensor(content_np, space).to(device)\n",
    "    style_full = np_to_tensor(style_np, space).to(device)\n",
    "\n",
    "    if content_mask is not None and style_mask is not None:\n",
    "        content_mask = content_mask.to(device)\n",
    "        style_mask = style_mask.to(device)\n",
    "\n",
    "    lr = 2e-3\n",
    "    extractor = Vgg19_Extractor(space=space).to(device)\n",
    "\n",
    "    scale_last = max(content_full.shape[2], content_full.shape[3])\n",
    "    scales = []\n",
    "    for scale in range(10):\n",
    "        divisor = 2**scale\n",
    "        if min(content_pil.width, content_pil.height) // divisor >= 33:\n",
    "            scales.insert(0, divisor)\n",
    "    \n",
    "    for scale in scales:\n",
    "        # rescale content to current scale\n",
    "        content = tensor_resample(content_full, [ content_full.shape[2] // scale, content_full.shape[3] // scale ])\n",
    "        style = tensor_resample(style_full, [ style_full.shape[2] // scale, style_full.shape[3] // scale ])\n",
    "        if AUTO_MASK_PYRAMID:\n",
    "            # using patch_match to get regions' correspondence\n",
    "            # Prom1: resample——因为原本的版本中，regions就只提供一次，大小和原图一样，但是现在我们是对每一个分辨率都得到一个新的regions\n",
    "            # Prom2: 我们需要将patch_match融入到我们代码当中——要研究对应的接口，希望最后能够得到两张图片正方形区域用颜色对应起来\n",
    "            feat1=extractor.patch_match_features(content)\n",
    "            feat2=extractor.patch_match_features(style)\n",
    "            pm=PatchMatch(feat1,feat2,3)\n",
    "            pm.propagate(iters=5)\n",
    "            matches=pm.nnf\n",
    "            im1=np_to_pil(tensor_to_np(content))\n",
    "            im2=np_to_pil(tensor_to_np(style))\n",
    "            regions=build_from_matches(matches,extractor.num_max_pool,im1,im2)\n",
    "            \n",
    "        print(f'Optimizing at resoluton [{content.shape[2]}, {content.shape[3]}]')\n",
    "        # upsample or initialize the result\n",
    "        if scale == scales[0]:\n",
    "            # first\n",
    "            result = laplacian(content) + style.mean(2,keepdim=True).mean(3,keepdim=True)\n",
    "        elif scale == scales[-1]:\n",
    "            # last \n",
    "            result = tensor_resample(result, [content.shape[2], content.shape[3]])\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            result = tensor_resample(result, [content.shape[2], content.shape[3]]) + laplacian(content)\n",
    "\n",
    "        # # do the optimization on this scale\n",
    "        result = optimize(result, content, style, content_path, style_path, content_weight=content_weight, lr=lr, extractor=extractor, regions=regions)\n",
    "\n",
    "        # next scale lower weight\n",
    "        content_weight /= 2.0\n",
    "\n",
    "    clow = -1.0 if space == 'uniform' else -1.7\n",
    "    chigh = 1.0 if space == 'uniform' else 1.7\n",
    "    result_image = tensor_to_np(tensor_resample(torch.clamp(result, clow, chigh), [content_full.shape[2], content_full.shape[3]])) # \n",
    "    # renormalize image\n",
    "    result_image -= result_image.min()\n",
    "    result_image /= result_image.max()\n",
    "    return np_to_pil(result_image * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/xiongjiangkai/Downloads/anni.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResulution too low.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     exit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m content_pil, style_pil \u001b[38;5;241m=\u001b[39m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, pil_loader(args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     15\u001b[0m content_mask, style_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyle_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_loader\u001b[39m(path):\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m         img \u001b[38;5;241m=\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/xiongjiangkai/Downloads/anni.jpg'"
     ]
    }
   ],
   "source": [
    "args={\"content\": \"content.jpg\", \"style\": \"style.jpg\", \"content_mask\": None, \"style_mask\": None, \"weight\": 1.0, \"output\": \"strotss.png\", \"device\": \"cuda:0\", \"ospace\": \"uniform\", \"resize_to\": 512,\"AUTO_MASK_PYRAMID\":False}\n",
    "args[\"content\"] =\"/Users/xiongjiangkai/Downloads/anni.jpg\"\n",
    "args[\"style\"] = \"/Users/xiongjiangkai/Downloads/mona.jpg\"\n",
    "args[\"output\"]=args[\"content\"].split('/')[-1].split('.')[0]+\"_\"+args[\"style\"].split('/')[-1].split('.')[0]+\"_masked.png\"\n",
    "AUTO_MASK_PYRAMID=args[\"AUTO_MASK_PYRAMID\"]=True\n",
    "# args[\"content_mask\"] =\"/Users/xiongjiangkai/Downloads/cv_heol/test_content/content_palace_single1.jpg\"\n",
    "# args[\"style_mask\"] = \"/Users/xiongjiangkai/Downloads/cv_heol/test_style/vangogh_starry_night_single1.jpg\"\n",
    "\n",
    "# make 256 the smallest possible long side, will still fail if short side is <\n",
    "if args[\"resize_to\"] < 2**8:\n",
    "    print(\"Resulution too low.\")\n",
    "    exit(1)\n",
    "\n",
    "content_pil, style_pil = pil_loader(args[\"content\"]), pil_loader(args[\"style\"])\n",
    "content_mask, style_mask = None, None\n",
    "\n",
    "if args[\"content_mask\"] and args[\"style_mask\"] is not None:\n",
    "    regions = extract_color_regions(args[\"content_mask\"], args[\"style_mask\"])\n",
    "\n",
    "    pil_content_mask = pil_loader(args[\"content_mask\"])\n",
    "    pil_style_mask = pil_loader(args[\"style_mask\"])\n",
    "    \n",
    "    pil_content_mask = pil_resize_long_edge_to(pil_content_mask, args[\"resize_to\"])\n",
    "    pil_style_mask = pil_resize_long_edge_to(pil_style_mask, args[\"resize_to\"])\n",
    "    \n",
    "    content_mask = pil_to_np(pil_content_mask)\n",
    "    style_mask = pil_to_np(pil_style_mask)\n",
    "\n",
    "    content_mask = create_mask_from_image(content_mask)\n",
    "    style_mask = create_mask_from_image(style_mask)\n",
    "else:\n",
    "    try:\n",
    "        regions = [[pil_to_np(pil_resize_long_edge_to(pil_loader(args[\"content\"]), args[\"resize_to\"]))[:,:,0]*0.+1.], [pil_to_np(pil_resize_long_edge_to(pil_loader(args[\"style\"]), args[\"resize_to\"]))[:,:,0]*0.+1.]]\n",
    "    except:\n",
    "        regions = [[pil_to_np(pil_resize_long_edge_to(pil_loader(args[\"content\"]), args[\"resize_to\"]))[:,:]*0.+1.], [pil_to_np(pil_resize_long_edge_to(pil_loader(args[\"style\"]), args[\"resize_to\"]))[:,:]*0.+1.]]\n",
    "\n",
    "content_weight = args[\"weight\"] * 16.0\n",
    "\n",
    "start = time()\n",
    "result = strotss(pil_resize_long_edge_to(content_pil, args[\"resize_to\"]), \n",
    "                    pil_resize_long_edge_to(style_pil, args[\"resize_to\"]), args[\"content\"], args[\"style\"], regions, content_weight, device, args[\"ospace\"], content_mask=content_mask, style_mask=style_mask)\n",
    "# result.save(args[\"output\"])\n",
    "print(f'Done in {time()-start:.3f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
