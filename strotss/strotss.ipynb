{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import PIL\n",
    "from time import time\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg16_Extractor(nn.Module):\n",
    "    def __init__(self, space):\n",
    "        super().__init__()\n",
    "        self.vgg_layers = models.vgg16(pretrained=True).features\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.capture_layers = [1,3,6,8,11,13,15,22,29]\n",
    "        self.space = space\n",
    "        \n",
    "    def forward_base(self, x):\n",
    "        feat = [x]\n",
    "        for i in range(len(self.vgg_layers)):\n",
    "            x = self.vgg_layers[i](x)\n",
    "            if i in self.capture_layers: feat.append(x)\n",
    "        return feat\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.space != 'vgg':\n",
    "            x = (x + 1.) / 2.\n",
    "            x = x - (torch.Tensor([0.485, 0.456, 0.406]).to(x.device).view(1, -1, 1, 1))\n",
    "            x = x / (torch.Tensor([0.229, 0.224, 0.225]).to(x.device).view(1, -1, 1, 1))\n",
    "        feat = self.forward_base(x)\n",
    "        return feat\n",
    "    \n",
    "    def forward_samples_hypercolumn(self, X, samps=100):\n",
    "        feat = self.forward(X)\n",
    "\n",
    "        xx,xy = np.meshgrid(np.arange(X.shape[2]), np.arange(X.shape[3]))\n",
    "        xx = np.expand_dims(xx.flatten(),1)\n",
    "        xy = np.expand_dims(xy.flatten(),1)\n",
    "        xc = np.concatenate([xx,xy],1)\n",
    "        \n",
    "        samples = min(samps,xc.shape[0])\n",
    "\n",
    "        np.random.shuffle(xc)\n",
    "        xx = xc[:samples,0]\n",
    "        yy = xc[:samples,1]\n",
    "\n",
    "        feat_samples = []\n",
    "        for i in range(len(feat)):\n",
    "\n",
    "            layer_feat = feat[i]\n",
    "\n",
    "            # hack to detect lower resolution\n",
    "            if i>0 and feat[i].size(2) < feat[i-1].size(2):\n",
    "                xx = xx/2.0\n",
    "                yy = yy/2.0\n",
    "\n",
    "            xx = np.clip(xx, 0, layer_feat.shape[2]-1).astype(np.int32)\n",
    "            yy = np.clip(yy, 0, layer_feat.shape[3]-1).astype(np.int32)\n",
    "\n",
    "            features = layer_feat[:,:, xx[range(samples)], yy[range(samples)]]\n",
    "            feat_samples.append(features.clone().detach())\n",
    "\n",
    "        feat = torch.cat(feat_samples,1)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor and PIL utils\n",
    "\n",
    "def pil_loader(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        img = PIL.Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def tensor_resample(tensor, dst_size, mode='bilinear'):\n",
    "    return F.interpolate(tensor, dst_size, mode=mode, align_corners=False)\n",
    "\n",
    "def pil_resize_short_edge_to(pil, trg_size):\n",
    "    short_w = pil.width < pil.height\n",
    "    ar_resized_short = (trg_size / pil.width) if short_w else (trg_size / pil.height)\n",
    "    resized = pil.resize((int(pil.width * ar_resized_short), int(pil.height * ar_resized_short)), PIL.Image.BICUBIC)\n",
    "    return resized\n",
    "\n",
    "def pil_resize_long_edge_to(pil, trg_size):\n",
    "    short_w = pil.width < pil.height\n",
    "    ar_resized_long = (trg_size / pil.height) if short_w else (trg_size / pil.width)\n",
    "    resized = pil.resize((int(pil.width * ar_resized_long), int(pil.height * ar_resized_long)), PIL.Image.BICUBIC)\n",
    "    return resized\n",
    "\n",
    "def np_to_pil(npy):\n",
    "    return PIL.Image.fromarray(npy.astype(np.uint8))\n",
    "\n",
    "def pil_to_np(pil):\n",
    "    return np.array(pil)\n",
    "\n",
    "def tensor_to_np(tensor, cut_dim_to_3=True):\n",
    "    if len(tensor.shape) == 4:\n",
    "        if cut_dim_to_3:\n",
    "            tensor = tensor[0]\n",
    "        else:\n",
    "            return tensor.data.cpu().numpy().transpose((0, 2, 3, 1))\n",
    "    return tensor.data.cpu().numpy().transpose((1,2,0))\n",
    "\n",
    "def np_to_tensor(npy, space):\n",
    "    if space == 'vgg':\n",
    "        return np_to_tensor_correct(npy)\n",
    "    return (torch.Tensor(npy.astype(np.float64) / 127.5) - 1.0).permute((2,0,1)).unsqueeze(0)\n",
    "\n",
    "def np_to_tensor_correct(npy):\n",
    "    pil = np_to_pil(npy)\n",
    "    transform = transforms.Compose([transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    return transform(pil).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laplacian Pyramid\n",
    "\n",
    "def laplacian(x):\n",
    "    # x - upsample(downsample(x))\n",
    "    return x - tensor_resample(tensor_resample(x, [x.shape[2] // 2, x.shape[3] // 2]), [x.shape[2], x.shape[3]])\n",
    "\n",
    "def make_laplace_pyramid(x, levels):\n",
    "    pyramid = []\n",
    "    current = x\n",
    "    for i in range(levels):\n",
    "        pyramid.append(laplacian(current))\n",
    "        current = tensor_resample(current, (max(current.shape[2] // 2,1), max(current.shape[3] // 2,1)))\n",
    "    pyramid.append(current)\n",
    "    return pyramid\n",
    "\n",
    "def fold_laplace_pyramid(pyramid):\n",
    "    current = pyramid[-1]\n",
    "    for i in range(len(pyramid)-2, -1, -1): # iterate from len-2 to 0\n",
    "        up_h, up_w = pyramid[i].shape[2], pyramid[i].shape[3]\n",
    "        current = pyramid[i] + tensor_resample(current, (up_h,up_w))\n",
    "    return current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_indices(feat_content, feat_style):\n",
    "    indices = None\n",
    "    const = 128**2 # 32k or so\n",
    "    feat_dims = feat_style.shape[1]\n",
    "    big_size = feat_content.shape[2] * feat_content.shape[3] # num feaxels\n",
    "\n",
    "    stride_x = int(max(math.floor(math.sqrt(big_size//const)),1))\n",
    "    offset_x = np.random.randint(stride_x)\n",
    "    stride_y = int(max(math.ceil(math.sqrt(big_size//const)),1))\n",
    "    offset_y = np.random.randint(stride_y)\n",
    "    xx, xy = np.meshgrid(np.arange(feat_content.shape[2])[offset_x::stride_x], np.arange(feat_content.shape[3])[offset_y::stride_y] )\n",
    "\n",
    "    xx = xx.flatten()\n",
    "    xy = xy.flatten()\n",
    "    return xx, xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_feature_extract(feat_result, feat_content, xx, xy):\n",
    "    l2, l3 = [], []\n",
    "    device = feat_result[0].device\n",
    "\n",
    "    # for each extracted layer\n",
    "    for i in range(len(feat_result)):\n",
    "        fr = feat_result[i]\n",
    "        fc = feat_content[i]\n",
    "\n",
    "        # hack to detect reduced scale\n",
    "        if i>0 and feat_result[i-1].size(2) > feat_result[i].size(2):\n",
    "            xx = xx/2.0\n",
    "            xy = xy/2.0\n",
    "\n",
    "        # go back to ints and get residual\n",
    "        xxm = np.floor(xx).astype(np.float32)\n",
    "        xxr = xx - xxm\n",
    "\n",
    "        xym = np.floor(xy).astype(np.float32)\n",
    "        xyr = xy - xym\n",
    "\n",
    "        # do bilinear resample\n",
    "        w00 = torch.from_numpy((1.-xxr)*(1.-xyr)).float().view(1, 1, -1, 1).to(device)\n",
    "        w01 = torch.from_numpy((1.-xxr)*xyr).float().view(1, 1, -1, 1).to(device)\n",
    "        w10 = torch.from_numpy(xxr*(1.-xyr)).float().view(1, 1, -1, 1).to(device)\n",
    "        w11 = torch.from_numpy(xxr*xyr).float().view(1, 1, -1, 1).to(device)\n",
    "\n",
    "        xxm = np.clip(xxm.astype(np.int32),0,fr.size(2)-1)\n",
    "        xym = np.clip(xym.astype(np.int32),0,fr.size(3)-1)\n",
    "\n",
    "        s00 = xxm*fr.size(3)+xym\n",
    "        s01 = xxm*fr.size(3)+np.clip(xym+1,0,fr.size(3)-1)\n",
    "        s10 = np.clip(xxm+1,0,fr.size(2)-1)*fr.size(3)+(xym)\n",
    "        s11 = np.clip(xxm+1,0,fr.size(2)-1)*fr.size(3)+np.clip(xym+1,0,fr.size(3)-1)\n",
    "\n",
    "        fr = fr.view(1,fr.size(1),fr.size(2)*fr.size(3),1)\n",
    "        fr = fr[:,:,s00,:].mul_(w00).add_(fr[:,:,s01,:].mul_(w01)).add_(fr[:,:,s10,:].mul_(w10)).add_(fr[:,:,s11,:].mul_(w11))\n",
    "\n",
    "        fc = fc.view(1,fc.size(1),fc.size(2)*fc.size(3),1)\n",
    "        fc = fc[:,:,s00,:].mul_(w00).add_(fc[:,:,s01,:].mul_(w01)).add_(fc[:,:,s10,:].mul_(w10)).add_(fc[:,:,s11,:].mul_(w11))\n",
    "\n",
    "        l2.append(fr)\n",
    "        l3.append(fc)\n",
    "\n",
    "    x_st = torch.cat([li.contiguous() for li in l2],1)\n",
    "    c_st = torch.cat([li.contiguous() for li in l3],1)\n",
    "\n",
    "    xx = torch.from_numpy(xx).view(1,1,x_st.size(2),1).float().to(device)\n",
    "    yy = torch.from_numpy(xy).view(1,1,x_st.size(2),1).float().to(device)\n",
    "    \n",
    "    x_st = torch.cat([x_st,xx,yy],1)\n",
    "    c_st = torch.cat([c_st,xx,yy],1)\n",
    "    return x_st, c_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances_cos(x, y):\n",
    "    x_norm = torch.sqrt((x**2).sum(1).view(-1, 1))\n",
    "    y_t = torch.transpose(y, 0, 1)\n",
    "    y_norm = torch.sqrt((y**2).sum(1).view(1, -1))\n",
    "    dist = 1.-torch.mm(x, y_t)/x_norm/y_norm\n",
    "    return dist\n",
    "\n",
    "def pairwise_distances_sq_l2(x, y):\n",
    "    x_norm = (x**2).sum(1).view(-1, 1)\n",
    "    y_t = torch.transpose(y, 0, 1)\n",
    "    y_norm = (y**2).sum(1).view(1, -1)\n",
    "    dist = x_norm + y_norm - 2.0 * torch.mm(x, y_t)\n",
    "    return torch.clamp(dist, 1e-5, 1e5)/x.size(1)\n",
    "\n",
    "def distmat(x, y, cos_d=True):\n",
    "    if cos_d:\n",
    "        M = pairwise_distances_cos(x, y)\n",
    "    else:\n",
    "        M = torch.sqrt(pairwise_distances_sq_l2(x, y))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(feat_result, feat_content):\n",
    "    d = feat_result.size(1)\n",
    "\n",
    "    X = feat_result.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "    Y = feat_content.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "\n",
    "    Y = Y[:,:-2]\n",
    "    X = X[:,:-2]\n",
    "    # X = X.t()\n",
    "    # Y = Y.t()\n",
    "\n",
    "    Mx = distmat(X, X)\n",
    "    Mx = Mx#/Mx.sum(0, keepdim=True)\n",
    "\n",
    "    My = distmat(Y, Y)\n",
    "    My = My#/My.sum(0, keepdim=True)\n",
    "\n",
    "    d = torch.abs(Mx-My).mean()# * X.shape[0]\n",
    "    return d\n",
    "\n",
    "def rgb_to_yuv(rgb):\n",
    "    C = torch.Tensor([[0.577350,0.577350,0.577350],[-0.577350,0.788675,-0.211325],[-0.577350,-0.211325,0.788675]]).to(rgb.device)\n",
    "    yuv = torch.mm(C,rgb)\n",
    "    return yuv\n",
    "\n",
    "def style_loss(X, Y, cos_d=True):\n",
    "    d = X.shape[1]\n",
    "\n",
    "    if d == 3:\n",
    "        X = rgb_to_yuv(X.transpose(0,1).contiguous().view(d,-1)).transpose(0,1)\n",
    "        Y = rgb_to_yuv(Y.transpose(0,1).contiguous().view(d,-1)).transpose(0,1)\n",
    "    else:\n",
    "        X = X.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "        Y = Y.transpose(0,1).contiguous().view(d,-1).transpose(0,1)\n",
    "\n",
    "    # Relaxed EMD\n",
    "    CX_M = distmat(X, Y, cos_d=True)\n",
    "\n",
    "    if d==3: CX_M = CX_M + distmat(X, Y, cos_d=False)\n",
    "\n",
    "    m1, m1_inds = CX_M.min(1)\n",
    "    m2, m2_inds = CX_M.min(0)\n",
    "\n",
    "    remd = torch.max(m1.mean(), m2.mean())\n",
    "\n",
    "    return remd\n",
    "\n",
    "def moment_loss(X, Y, moments=[1,2]):\n",
    "    loss = 0.\n",
    "    X = X.squeeze().t()\n",
    "    Y = Y.squeeze().t()\n",
    "\n",
    "    mu_x = torch.mean(X, 0, keepdim=True)\n",
    "    mu_y = torch.mean(Y, 0, keepdim=True)\n",
    "    mu_d = torch.abs(mu_x - mu_y).mean()\n",
    "\n",
    "    if 1 in moments:\n",
    "        # print(mu_x.shape)\n",
    "        loss = loss + mu_d\n",
    "\n",
    "    if 2 in moments:\n",
    "        X_c = X - mu_x\n",
    "        Y_c = Y - mu_y\n",
    "        X_cov = torch.mm(X_c.t(), X_c) / (X.shape[0] - 1)\n",
    "        Y_cov = torch.mm(Y_c.t(), Y_c) / (Y.shape[0] - 1)\n",
    "\n",
    "        # print(X_cov.shape)\n",
    "        # exit(1)\n",
    "\n",
    "        D_cov = torch.abs(X_cov - Y_cov).mean()\n",
    "        loss = loss + D_cov\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(feat_result, feat_content, feat_style, indices, content_weight, moment_weight=1.0):\n",
    "    # spatial feature extract\n",
    "    num_locations = 1024\n",
    "    spatial_result, spatial_content = spatial_feature_extract(feat_result, feat_content, indices[0][:num_locations], indices[1][:num_locations])\n",
    "    loss_content = content_loss(spatial_result, spatial_content)\n",
    "\n",
    "    d = feat_style.shape[1]\n",
    "    spatial_style = feat_style.view(1, d, -1, 1)\n",
    "    feat_max = 3+2*64+128*2+256*3+512*2 # (sum of all extracted channels)\n",
    "\n",
    "    loss_remd = style_loss(spatial_result[:, :feat_max, :, :], spatial_style[:, :feat_max, :, :])\n",
    "\n",
    "    loss_moment = moment_loss(spatial_result[:,:-2,:,:], spatial_style, moments=[1,2]) # -2 is so that it can fit?\n",
    "    # palette matching\n",
    "    content_weight_frac = 1./max(content_weight,1.)\n",
    "    loss_moment += content_weight_frac * style_loss(spatial_result[:,:3,:,:], spatial_style[:,:3,:,:])\n",
    "    \n",
    "    loss_style = loss_remd + moment_weight * loss_moment\n",
    "    # print(f'Style: {loss_style.item():.3f}, Content: {loss_content.item():.3f}')\n",
    "\n",
    "    style_weight = 1.0 + moment_weight\n",
    "    loss_total = (content_weight * loss_content + loss_style) / (content_weight + style_weight)\n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(result, content, style, scale, content_weight, lr, extractor):\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    result_pyramid = make_laplace_pyramid(result, 5)\n",
    "    result_pyramid = [l.data.requires_grad_() for l in result_pyramid]\n",
    "\n",
    "    opt_iter = 200\n",
    "    # if scale == 1:\n",
    "    #     opt_iter = 800\n",
    "\n",
    "    # use rmsprop\n",
    "    optimizer = optim.RMSprop(result_pyramid, lr=lr)\n",
    "\n",
    "    # extract features for content\n",
    "    feat_content = extractor(content) # \n",
    "\n",
    "    stylized = fold_laplace_pyramid(result_pyramid)\n",
    "    # let's ignore the regions for now\n",
    "    # some inner loop that extracts samples\n",
    "    feat_style = None\n",
    "    for i in range(5):\n",
    "        with torch.no_grad():\n",
    "            # r is region of interest (mask)\n",
    "            feat_e = extractor.forward_samples_hypercolumn(style, samps=1000)\n",
    "            feat_style = feat_e if feat_style is None else torch.cat((feat_style, feat_e), dim=2)\n",
    "    # feat_style.requires_grad_(False)\n",
    "\n",
    "    # init indices to optimize over\n",
    "    xx, xy = sample_indices(feat_content[0], feat_style) # 0 to sample over first layer extracted\n",
    "    for it in range(opt_iter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        stylized = fold_laplace_pyramid(result_pyramid)\n",
    "        # original code has resample here, seems pointless with uniform shuffle\n",
    "        # ...\n",
    "        # also shuffle them every y iter\n",
    "        if it % 1 == 0 and it != 0:\n",
    "            np.random.shuffle(xx)\n",
    "            np.random.shuffle(xy)\n",
    "        feat_result = extractor(stylized)\n",
    "\n",
    "        loss = calculate_loss(feat_result, feat_content, feat_style, [xx, xy], content_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return stylized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strotss(content_pil, style_pil, content_weight=1.0*16.0, device='cuda:0', space='uniform'):\n",
    "    content_np = pil_to_np(content_pil)\n",
    "    style_np = pil_to_np(style_pil)\n",
    "    content_full = np_to_tensor(content_np, space).to(device)\n",
    "    style_full = np_to_tensor(style_np, space).to(device)\n",
    "\n",
    "    lr = 2e-3\n",
    "    extractor = Vgg16_Extractor(space=space).to(device)\n",
    "\n",
    "    scale_last = max(content_full.shape[2], content_full.shape[3])\n",
    "    scales = []\n",
    "    for scale in range(10):\n",
    "        divisor = 2**scale\n",
    "        if min(content_pil.width, content_pil.height) // divisor >= 33:\n",
    "            scales.insert(0, divisor)\n",
    "    \n",
    "    for scale in scales:\n",
    "        # rescale content to current scale\n",
    "        content = tensor_resample(content_full, [ content_full.shape[2] // scale, content_full.shape[3] // scale ])\n",
    "        style = tensor_resample(style_full, [ style_full.shape[2] // scale, style_full.shape[3] // scale ])\n",
    "        print(f'Optimizing at resoluton [{content.shape[2]}, {content.shape[3]}]')\n",
    "\n",
    "        # upsample or initialize the result\n",
    "        if scale == scales[0]:\n",
    "            # first\n",
    "            result = laplacian(content) + style.mean(2,keepdim=True).mean(3,keepdim=True)\n",
    "        elif scale == scales[-1]:\n",
    "            # last \n",
    "            result = tensor_resample(result, [content.shape[2], content.shape[3]])\n",
    "            lr = 1e-3\n",
    "        else:\n",
    "            result = tensor_resample(result, [content.shape[2], content.shape[3]]) + laplacian(content)\n",
    "\n",
    "        # do the optimization on this scale\n",
    "        result = optimize(result, content, style, scale, content_weight=content_weight, lr=lr, extractor=extractor)\n",
    "\n",
    "        # next scale lower weight\n",
    "        content_weight /= 2.0\n",
    "\n",
    "    clow = -1.0 if space == 'uniform' else -1.7\n",
    "    chigh = 1.0 if space == 'uniform' else 1.7\n",
    "    result_image = tensor_to_np(tensor_resample(torch.clamp(result, clow, chigh), [content_full.shape[2], content_full.shape[3]])) # \n",
    "    # renormalize image\n",
    "    result_image -= result_image.min()\n",
    "    result_image /= result_image.max()\n",
    "    return np_to_pil(result_image * 255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing at resoluton [47, 63]\n",
      "Optimizing at resoluton [95, 127]\n",
      "Optimizing at resoluton [191, 255]\n",
      "Optimizing at resoluton [383, 511]\n",
      "Done in 20.792s\n"
     ]
    }
   ],
   "source": [
    "args = {\"content\": \"/root/cv_heol/test_content/lake.jpg\", \"style\": \"/root/cv_heol/test_style/monet.jpg\", \"weight\": 1.0, \"output\": \"strotss.png\", \"device\": \"cuda:0\", \"ospace\": \"uniform\", \"resize_to\": 512}\n",
    "\n",
    "# make 256 the smallest possible long side, will still fail if short side is <\n",
    "if args[\"resize_to\"]< 2**8:\n",
    "    print(\"Resulution too low.\")\n",
    "    exit(1)\n",
    "\n",
    "args[\"output\"]=args[\"content\"].split('/')[-1].split('.')[0] +\"_\"+ args[\"style\"].split('/')[-1].split('.')[0] + '.png'\n",
    "content_pil, style_pil = pil_loader(args[\"content\"]), pil_loader(args[\"style\"])\n",
    "content_weight = args[\"weight\"] * 16.0\n",
    "\n",
    "device = args[\"device\"]\n",
    "\n",
    "start = time()\n",
    "result = strotss(pil_resize_long_edge_to(content_pil, args[\"resize_to\"]), \n",
    "                    pil_resize_long_edge_to(style_pil, args[\"resize_to\"]), content_weight, device, args[\"ospace\"])\n",
    "result.save(args[\"output\"])\n",
    "print(f'Done in {time()-start:.3f}s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
